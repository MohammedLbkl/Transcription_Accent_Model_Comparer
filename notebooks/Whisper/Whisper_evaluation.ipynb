{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import whisper\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from jiwer import wer\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "from src.audio.audio import Ecoute_audio\n",
    "from src.models.evaluate_model import Whisper_evaluate\n",
    "from src.models.transcription_model import whisper_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>duration[ms]</th>\n",
       "      <th>wav_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_en_40865211.mp3</td>\n",
       "      <td>With this transition to the big time, the band...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>5904</td>\n",
       "      <td>common_voice_en_40865211.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_en_40865212.mp3</td>\n",
       "      <td>Local brothels recruited extra staff to cope w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>5544</td>\n",
       "      <td>common_voice_en_40865212.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_en_40865213.mp3</td>\n",
       "      <td>With Fox on lead vocals, the threesome did two...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>5760</td>\n",
       "      <td>common_voice_en_40865213.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_en_40865214.mp3</td>\n",
       "      <td>Miramax requested cuts be made and Christopher...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>5652</td>\n",
       "      <td>common_voice_en_40865214.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_en_40865215.mp3</td>\n",
       "      <td>The Key allows customers to buy Plusbus for th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States English</td>\n",
       "      <td>6120</td>\n",
       "      <td>common_voice_en_40865215.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18321</th>\n",
       "      <td>common_voice_en_41227190.mp3</td>\n",
       "      <td>Bolton's wife is named Liliana; they have two ...</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>Scottish English</td>\n",
       "      <td>9756</td>\n",
       "      <td>common_voice_en_41227190.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18322</th>\n",
       "      <td>common_voice_en_41227191.mp3</td>\n",
       "      <td>One report indicates that they formerly spoke ...</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>Scottish English</td>\n",
       "      <td>10296</td>\n",
       "      <td>common_voice_en_41227191.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18323</th>\n",
       "      <td>common_voice_en_41227192.mp3</td>\n",
       "      <td>He was fascinated by topics including photogra...</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>Scottish English</td>\n",
       "      <td>9576</td>\n",
       "      <td>common_voice_en_41227192.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18324</th>\n",
       "      <td>common_voice_en_41227193.mp3</td>\n",
       "      <td>Just some Galley Push-Ups.</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>Scottish English</td>\n",
       "      <td>5148</td>\n",
       "      <td>common_voice_en_41227193.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18325</th>\n",
       "      <td>common_voice_en_41227194.mp3</td>\n",
       "      <td>I like 'spontaneous' as a sexual description.</td>\n",
       "      <td>fourties</td>\n",
       "      <td>female_feminine</td>\n",
       "      <td>Scottish English</td>\n",
       "      <td>7020</td>\n",
       "      <td>common_voice_en_41227194.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18326 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path  \\\n",
       "0      common_voice_en_40865211.mp3   \n",
       "1      common_voice_en_40865212.mp3   \n",
       "2      common_voice_en_40865213.mp3   \n",
       "3      common_voice_en_40865214.mp3   \n",
       "4      common_voice_en_40865215.mp3   \n",
       "...                             ...   \n",
       "18321  common_voice_en_41227190.mp3   \n",
       "18322  common_voice_en_41227191.mp3   \n",
       "18323  common_voice_en_41227192.mp3   \n",
       "18324  common_voice_en_41227193.mp3   \n",
       "18325  common_voice_en_41227194.mp3   \n",
       "\n",
       "                                                sentence       age  \\\n",
       "0      With this transition to the big time, the band...       NaN   \n",
       "1      Local brothels recruited extra staff to cope w...       NaN   \n",
       "2      With Fox on lead vocals, the threesome did two...       NaN   \n",
       "3      Miramax requested cuts be made and Christopher...       NaN   \n",
       "4      The Key allows customers to buy Plusbus for th...       NaN   \n",
       "...                                                  ...       ...   \n",
       "18321  Bolton's wife is named Liliana; they have two ...  fourties   \n",
       "18322  One report indicates that they formerly spoke ...  fourties   \n",
       "18323  He was fascinated by topics including photogra...  fourties   \n",
       "18324                         Just some Galley Push-Ups.  fourties   \n",
       "18325      I like 'spontaneous' as a sexual description.  fourties   \n",
       "\n",
       "                gender                accents  duration[ms]  \\\n",
       "0                  NaN  United States English          5904   \n",
       "1                  NaN  United States English          5544   \n",
       "2                  NaN  United States English          5760   \n",
       "3                  NaN  United States English          5652   \n",
       "4                  NaN  United States English          6120   \n",
       "...                ...                    ...           ...   \n",
       "18321  female_feminine       Scottish English          9756   \n",
       "18322  female_feminine       Scottish English         10296   \n",
       "18323  female_feminine       Scottish English          9576   \n",
       "18324  female_feminine       Scottish English          5148   \n",
       "18325  female_feminine       Scottish English          7020   \n",
       "\n",
       "                           wav_path  \n",
       "0      common_voice_en_40865211.wav  \n",
       "1      common_voice_en_40865212.wav  \n",
       "2      common_voice_en_40865213.wav  \n",
       "3      common_voice_en_40865214.wav  \n",
       "4      common_voice_en_40865215.wav  \n",
       "...                             ...  \n",
       "18321  common_voice_en_41227190.wav  \n",
       "18322  common_voice_en_41227191.wav  \n",
       "18323  common_voice_en_41227192.wav  \n",
       "18324  common_voice_en_41227193.wav  \n",
       "18325  common_voice_en_41227194.wav  \n",
       "\n",
       "[18326 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../../Data/raw/other.tsv\", sep=\"\\t\")\n",
    "df1 = df1[[\"path\",\"sentence\",\"age\",\"gender\",\"accents\"]]\n",
    "df2 = pd.read_csv(\"../../Data/raw/clip_durations.tsv\", sep=\"\\t\")\n",
    "df2 = df2.rename(columns={'clip': 'path'})\n",
    "\n",
    "metadata = pd.merge(df1, df2, on='path')\n",
    "metadata['wav_path'] = metadata['path'].str.replace('.mp3', '.wav', regex=False)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : 0\n",
      "Age : 6181\n",
      "Gender : 7759\n",
      "Accents : 4160\n",
      "Duration : 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Sentence : {metadata[\"sentence\"].isna().sum()}')\n",
    "print(f'Age : {metadata[\"age\"].isna().sum()}')\n",
    "print(f'Gender : {metadata[\"gender\"].isna().sum()}')\n",
    "print(f'Accents : {metadata[\"accents\"].isna().sum()}')\n",
    "print(f'Duration : {metadata[\"duration[ms]\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['United States English' nan 'Australian English']\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(metadata['accents'].unique()[:3])\n",
    "print(len(metadata['accents'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'fourties' 'thirties' 'twenties' 'teens' 'sixties' 'fifties']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(metadata['age'].unique())\n",
    "print(len(metadata['age'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'male_masculine' 'female_feminine' 'non-binary' 'do_not_wish_to_say'\n",
      " 'transgender']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(metadata['gender'].unique())\n",
    "print(len(metadata['gender'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata.to_csv('../../data/processed/Common Voice.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=metadata[[\"path\",\"sentence\",\"wav_path\"]]\n",
    "\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"../../clips.wav/common_voice_en_40865212.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<audio controls>\n",
       "  <source src=\"../../clips.wav/common_voice_en_40865212.wav\" type=\"audio/mpeg\">\n",
       "  Votre navigateur ne supporte pas la balise audio.\n",
       "</audio>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = AUDIO_PATH\n",
    "  \n",
    "Ecoute_audio(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Local brothels recruited extra staff to cope with the increase in business.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(AUDIO_PATH)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local brothels recruited extra staff to cope with the increase in business.\n"
     ]
    }
   ],
   "source": [
    "resulte = whisper_transcription(file_path=AUDIO_PATH, model_name=\"tiny\")\n",
    "print(resulte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 37.50%\n"
     ]
    }
   ],
   "source": [
    "reference = \"Evaluons sa capacitÃ© Ã  transcrire correctement une phrase.\"\n",
    "hypothesis = \"Evaluons sa capacitÃ© Ã  transcrir correctement une phrases complexe\"\n",
    "\n",
    "score = wer(reference, hypothesis)\n",
    "print(f\"WER: {score:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tiny = whisper.load_model(\"tiny\")\n",
    "model_base = whisper.load_model(\"base\")\n",
    "model_small = whisper.load_model(\"small\")\n",
    "model_medium = whisper.load_model(\"medium\")\n",
    "model_turbo = whisper.load_model(\"turbo\")\n",
    "model_turbo = whisper.load_model(\"large\")\n",
    "\n",
    "list_models = [\"tiny\", \"base\", \"small\", \"medium\", \"turbo\",\"large\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tiny\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m list_models:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mWhisper_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtiny\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\src\\models\\evaluate_model.py:29\u001b[39m, in \u001b[36mWhisper_evaluate\u001b[39m\u001b[34m(model_name, data)\u001b[39m\n\u001b[32m     25\u001b[39m mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\u001b[32m     28\u001b[39m options = whisper.DecodingOptions()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m result = \u001b[43mwhisper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m end = time.time()\n\u001b[32m     33\u001b[39m duration = end - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\whisper\\decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\whisper\\decoding.py:718\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    715\u001b[39m tokenizer: Tokenizer = \u001b[38;5;28mself\u001b[39m.tokenizer\n\u001b[32m    716\u001b[39m n_audio: \u001b[38;5;28mint\u001b[39m = mel.shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m audio_features: Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# encoder forward pass\u001b[39;00m\n\u001b[32m    719\u001b[39m tokens: Tensor = torch.tensor([\u001b[38;5;28mself\u001b[39m.initial_tokens]).repeat(n_audio, \u001b[32m1\u001b[39m)\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# detect language if requested, overwriting the language token\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\whisper\\decoding.py:655\u001b[39m, in \u001b[36mDecodingTask._get_audio_features\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    653\u001b[39m     audio_features = mel\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     audio_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio_features.dtype != (\n\u001b[32m    658\u001b[39m     torch.float16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.fp16 \u001b[38;5;28;01melse\u001b[39;00m torch.float32\n\u001b[32m    659\u001b[39m ):\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    661\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maudio_features has an incorrect dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_features.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    662\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\whisper\\model.py:194\u001b[39m, in \u001b[36mAudioEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mx : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03m    the mel spectrogram of the audio\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    193\u001b[39m x = F.gelu(\u001b[38;5;28mself\u001b[39m.conv1(x))\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m x = F.gelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    195\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m x.shape[\u001b[32m1\u001b[39m:] == \u001b[38;5;28mself\u001b[39m.positional_embedding.shape, \u001b[33m\"\u001b[39m\u001b[33mincorrect audio shape\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\whisper\\model.py:57\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, x, weight, bias)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conv_forward\u001b[39m(\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n\u001b[32m     56\u001b[39m ) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Simoh\\Transcription_Accent_Model_Comparer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    360\u001b[39m         F.pad(\n\u001b[32m    361\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for model_name in list_models:\n",
    "    Whisper_evaluate(model_name = \"tiny\", data = metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
